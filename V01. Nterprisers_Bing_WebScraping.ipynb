{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Web Scraping through Bing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Graduate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Web Scraping of Company Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the DataFrame with company and description columns\n",
    "df = pd.read_csv(\"C:/Users/Graduate/Desktop/Grad MSBA/Advanced Python/Final Project/Test Dataset/company_info_final - company_info_final.csv\")\n",
    "\n",
    "# Load the NAICS_subcategories file\n",
    "naics_subcategories = pd.read_csv(\"NAICS_subcategories.csv\")\n",
    "\n",
    "# Merge the two DataFrames on the first 3 digits of the NAICS code\n",
    "df = pd.merge(df, naics_subcategories, left_on=df[\"NAICS on SoS site\"].astype(str).str[:3], right_on=naics_subcategories[\"NAICS Code\"].astype(str).str[:3], how=\"left\")\n",
    "\n",
    "# Define an empty dictionary to store the results\n",
    "keyword_dict = {}\n",
    "\n",
    "# Add keyword_count and found_count columns to the DataFrame\n",
    "df[\"keyword_count\"] = 0\n",
    "df[\"found_count\"] = 0\n",
    "\n",
    "# Add a \"Manufacturing\" column to the DataFrame and set it to False\n",
    "df[\"Manufacturing\"] = False\n",
    "\n",
    "# Create a new instance of the Firefox driver\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Define a set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Loop through all rows in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the NAICS Code is blank\n",
    "    if pd.isnull(row[\"NAICS on SoS site\"]):\n",
    "        continue  # skip to next row\n",
    "    \n",
    "    company = row[\"Company\"].strip()\n",
    "    description = row[\"Description\"]\n",
    "    \n",
    "    company_name = company.replace(\"&\", \"and\")\n",
    "        \n",
    "    # Navigate to bing.com with the search query\n",
    "    driver.get(\"https://www.bing.com/search?q=\" + company_name + \" \" + \"manufacturing\" + \" \" + \"Rhode Island\")\n",
    "        \n",
    "    # Wait for the search results to load\n",
    "    driver.implicitly_wait(10)\n",
    "        \n",
    "    # Find the first search result link that is not an advertisement\n",
    "    search_results = driver.find_elements('css selector', \"div.tF2Cxc\")\n",
    "    for result in search_results:\n",
    "        try:\n",
    "            search_link = result.find_element('tag name', 'a')\n",
    "            if 'http' in search_link.get_attribute('href') and 'google' not in search_link.get_attribute('href'):\n",
    "                search_link.click()\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Wait for the resulting page to load\n",
    "    driver.implicitly_wait(10)\n",
    "        \n",
    "    # Check if the description is blank\n",
    "    if pd.isnull(description):\n",
    "        df.at[index, \"Manufacturing\"] = False\n",
    "        continue\n",
    "        \n",
    "    # Get the webpage content\n",
    "    webpage = driver.page_source\n",
    "        \n",
    "    # Loop through each keyword for the current company\n",
    "    for word in description.split():        \n",
    "        if word.lower() not in stop_words:\n",
    "            keyword_dict.setdefault(word.lower(), []).append(1 if word.lower() in webpage.lower() else 0)\n",
    "            # Increment the found count for the current row if the keyword is found in the webpage\n",
    "            df.at[index, \"found_count\"] += 1 if word.lower() in webpage.lower() else 0\n",
    "            # Increment the keyword count for the current row\n",
    "            df.at[index, \"keyword_count\"] += 1\n",
    "        else:\n",
    "            keyword_dict.setdefault(word.lower(), []).append(0)\n",
    "\n",
    "        if word.lower() == 'manufacturing':\n",
    "            df.at[index, \"Manufacturing\"] = True\n",
    "\n",
    "    # Add the found count to the DataFrame\n",
    "    df.at[index, \"found_count\"] = df.at[index, \"found_count\"]\n",
    "    df.at[index, \"Manufacturing\"] = df.at[index, \"Manufacturing\"]\n",
    "        \n",
    "    # Calculate the percentage of keywords found and add to the DataFrame\n",
    "    percentage = df.at[index, \"found_count\"] / df.at[index, \"keyword_count\"] * 100 if df.at[index, \"keyword_count\"] > 0 else 0\n",
    "    df.at[index, \"found_percentage\"] = \"{:.2f}%\".format(percentage)\n",
    "        \n",
    "# Close the web driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Excel file and place it in the \"Web Scrape\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeb Scrape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# drop the first column\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#df = df.drop(df.columns[0], axis=1)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save the updated DataFrame to a new CSV file in the Web Scrape folder\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeb Scrape/google_web_scrape.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(\"Web Scrape\"):\n",
    "    os.makedirs(\"Web Scrape\")\n",
    "\n",
    "# drop the first column\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file in the Web Scrape folder\n",
    "df.to_csv(\"Web Scrape/bing_web_scrape.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
