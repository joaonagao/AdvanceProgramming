{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Web Scraping through LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Graduate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert .txt file into .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGraduate\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGrad MSBA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAdvanced Python\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFinal Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcompany_info_final.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile, \\\n\u001b[0;32m      2\u001b[0m         \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_info_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m----> 3\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(outfile)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Write the header row\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNAICS on SoS site\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\Users\\Graduate\\Desktop\\Grad MSBA\\Advanced Python\\Final Project\\company_info_final.txt', 'r') as infile, \\\n",
    "    open('company_info_final.csv', 'w', newline='') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Company', 'NAICS on SoS site', 'Principal Address'])\n",
    "\n",
    "    # Loop through the lines in the input file\n",
    "    fields = {}\n",
    "    for line in infile:\n",
    "        # Strip whitespace from the line\n",
    "        line = line.strip()\n",
    "\n",
    "        # If the line starts with \"Company:\", start a new record\n",
    "        if line.startswith('Company:'):\n",
    "            # Add the previous record to the output file, if any\n",
    "            if fields:\n",
    "                # Remove non-numeric characters from NAICS code\n",
    "                naics = fields.get('NAICS on SoS site', '')\n",
    "                naics = re.sub(r'\\D', '', naics)\n",
    "                # Remove anything that comes after \" RI \" in the Principal Address field\n",
    "                principal_address = fields.get('Principal Address', '')\n",
    "                principal_address = re.sub(r' RI .*', ' RI', principal_address)\n",
    "                # Replace \", ,\" with \",\" in the Principal Address field\n",
    "                principal_address = principal_address.replace(', ,', ',')\n",
    "                writer.writerow([fields.get('Company', ''), naics, principal_address])\n",
    "\n",
    "            # Start a new dictionary of fields\n",
    "            fields = {}\n",
    "\n",
    "        # Split the line into fields based on the delimiter\n",
    "        if ':' in line:\n",
    "            key, value = [s.strip() for s in line.split(':', 1)]\n",
    "            if key == 'NAICS on SoS site':\n",
    "                fields[key] = value.strip()\n",
    "            elif key == 'Principal Address':\n",
    "                next(infile)  # skip the next line\n",
    "                fields[key] = next(infile).strip()\n",
    "            else:\n",
    "                fields[key] = value\n",
    "\n",
    "    # Write the last record to the output file\n",
    "    if fields:\n",
    "        # Remove non-numeric characters from NAICS code\n",
    "        naics = fields.get('NAICS on SoS site', '')\n",
    "        naics = re.sub(r'\\D', '', naics)\n",
    "        # Remove anything that comes after \" RI \" in the Principal Address field\n",
    "        principal_address = fields.get('Principal Address', '')\n",
    "        principal_address = re.sub(r' RI .*', ' RI', principal_address)\n",
    "        # Replace \", ,\" with \",\" in the Principal Address field\n",
    "        principal_address = principal_address.replace(', ,', ',')\n",
    "        writer.writerow([fields.get('Company', ''), naics, principal_address])\n",
    "\n",
    "print('CSV file generated successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Web Scraping of Company Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the DataFrame with company and description columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Graduate/Desktop/Grad MSBA/Advanced Python/Final Project/company_info_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the NAICS_subcategories file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m naics_subcategories \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAICS_subcategories.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the DataFrame with company and description columns\n",
    "df = pd.read_csv(\"C:/Users/Graduate/Desktop/Grad MSBA/Advanced Python/Final Project/company_info_final.csv\")\n",
    "\n",
    "# Load the NAICS_subcategories file\n",
    "naics_subcategories = pd.read_csv(\"NAICS_subcategories.csv\")\n",
    "\n",
    "# Merge the two DataFrames on the first 3 digits of the NAICS code\n",
    "df = pd.merge(df, naics_subcategories, left_on=df[\"NAICS on SoS site\"].astype(str).str[:3], right_on=naics_subcategories[\"NAICS Code\"].astype(str).str[:3], how=\"left\")\n",
    "\n",
    "# Define an empty dictionary to store the results\n",
    "keyword_dict = {}\n",
    "\n",
    "# Define a set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add keyword_count and found_count columns to the DataFrame\n",
    "df[\"keyword_count\"] = 0\n",
    "df[\"found_count\"] = 0\n",
    "\n",
    "# Add a \"Manufacturing\" column to the DataFrame\n",
    "df[\"Manufacturing\"] = False\n",
    "\n",
    "# Start the web driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Loop through all rows in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the NAICS Code is blank\n",
    "    if pd.isnull(row[\"NAICS on SoS site\"]):\n",
    "        continue  # skip to next row\n",
    "    \n",
    "    company = row[\"Company\"].strip()\n",
    "    description = row[\"Description\"]\n",
    "    address = row[\"Principal Address\"]\n",
    "    \n",
    "    company_name = company.replace(\"&\", \"and\")\n",
    "        \n",
    "    # Navigate to Yelp.com with the search query\n",
    "    driver.get(\"https://www.yelp.com/\")\n",
    "    search_box = driver.find_element_by_name(\"find_desc\")\n",
    "    search_box.send_keys(company_name + \" \" + \"manufacturing\")\n",
    "    search_box.submit()\n",
    "        \n",
    "    # Wait for the search results to load\n",
    "    driver.implicitly_wait(10)\n",
    "        \n",
    "    # Find the first search result link\n",
    "    search_results = driver.find_elements_by_css_selector(\"h4 > a\")\n",
    "    if len(search_results) > 0:\n",
    "        search_link = search_results[0]\n",
    "        search_link.click()\n",
    "            \n",
    "        # Wait for the resulting page to load\n",
    "        driver.implicitly_wait(10)\n",
    "            \n",
    "        # Extract the relevant information from the page\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "        # Loop through each keyword for the current company\n",
    "        for word in (description).split():\n",
    "            if word.lower() not in stop_words:\n",
    "                if word.lower() == 'manufacturing':\n",
    "                    df.at[index, \"Manufacturing\"] = True\n",
    "                if word.lower() in page_source.lower():\n",
    "                    keyword_dict.setdefault(word.lower(), []).append(1)\n",
    "                    # Increment the found count for the current row\n",
    "                    df.at[index, \"found_count\"] += 1\n",
    "                else:\n",
    "                    keyword_dict.setdefault(word.lower(), []).append(0)\n",
    "                # Increment the keyword count for the current row\n",
    "                df.at[index, \"keyword_count\"] += 1 \n",
    "                    \n",
    "        # Add the found count to the DataFrame\n",
    "        df.at[index, \"found_count\"] = df.at[index, \"found_count\"]\n",
    "        df.at[index, \"Manufacturing\"] = df.at[index, \"Manufacturing\"]\n",
    "            \n",
    "        # Calculate the percentage of keywords found and add to the DataFrame\n",
    "        percentage = df.at[index, \"found_count\"] / df.at[index, \"keyword_count\"] if df.at[index, \"keyword_count\"] > 0 else 0\n",
    "        df.at[index, \"found_percentage\"] = percentage\n",
    "                \n",
    "# Close the web driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Excel file and place it in the \"Web Scrape\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(\"Web Scrape\"):\n",
    "    os.makedirs(\"Web Scrape\")\n",
    "\n",
    "# drop the first column\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file in the Web Scrape folder\n",
    "df.to_csv(\"Web Scrape/yelp_web_scrape.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
